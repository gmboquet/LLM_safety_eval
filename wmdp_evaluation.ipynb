{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c550255",
   "metadata": {},
   "source": [
    "# Translating and Evaluating Questions from the WMDP Dataset\n",
    "\n",
    "Grant Boquet, boquet1@llnl.gov\n",
    "\n",
    "1/17/2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b66336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\" # PUT YOUR OPENAI KEY HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f1e2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import csv\n",
    "import json\n",
    "\n",
    "from uuid import UUID\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "from langchain_core.outputs.llm_result import LLMResult\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564d3aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "evaluation_model = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1782248b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"cais/wmdp\", \"wmdp-chem\")\n",
    "all_entries = []\n",
    "for entry in ds[\"test\"]:\n",
    "    all_entries.append({\"en_question\": entry[\"question\"], \"en_choices\": json.dumps(entry[\"choices\"], ensure_ascii=False), \"answer\": entry[\"answer\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347fd8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchCallback(BaseCallbackHandler):\n",
    "    def __init__(self, total: int):\n",
    "        super().__init__()\n",
    "        self.count = 0\n",
    "        self.progress_bar = tqdm(total=total)\n",
    "\n",
    "    def on_llm_end(self, response: LLMResult, *, run_id: UUID, parent_run_id: UUID | None = None, **kwargs: Any) -> Any:\n",
    "        self.count += 1\n",
    "        self.progress_bar.update(1)\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.progress_bar.__enter__()\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_value, exc_traceback):\n",
    "        self.progress_bar.__exit__(exc_type, exc_value, exc_traceback)\n",
    "\n",
    "    def __del__(self):\n",
    "        self.progress_bar.__del__()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7ab06e",
   "metadata": {},
   "source": [
    "## Translation Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c613f8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translation(TypedDict):\n",
    "    trad_zh_question: Annotated[str, ..., \"Translation of the English question, ENGLISH_QUESTION, that is a faithful representations of the same concepts and written as how a Taiwanese chemist would write the same question\"]\n",
    "    trad_zh_choices: Annotated[List[str], ..., \"List of strings with possible answers to the Traditional Chinese question that is a translation of the English choices, ENGLISH_CHOICES, and is a faithful representations of the same concepts and written as how a Taiwanese chemist would write the same list of choices\"]\n",
    "    en_question: Annotated[str, ..., \"Question in English\"]\n",
    "    en_choices: Annotated[List[str], ..., \"List of strings with possible answers to the English question\"]\n",
    "\n",
    "\n",
    "system_template = \"\"\"You are a chemistry professor creating an exam that will be taken by both Taiwanese and American students. You want to ensure the exam is fair, so each of the questions asked on this exam must be a faithful representation of the same concepts across both languages. English question, ENGLISH_QUESTION, and possible English answer choices, ENGLISH_CHOICES, are faithful representations of the same concepts and would be how a professional Taiwanese chemist would word the same question and answers.\"\"\"\n",
    "\n",
    "human_template = \"\"\"\n",
    "ENGLISH_QUESTION: {en_question}\n",
    "ENGLISH_CHOICES: {en_choices}\n",
    "\"\"\"\n",
    "\n",
    "translation_prompt = ChatPromptTemplate.from_messages([(\"system\", system_template), (\"human\", human_template)])\n",
    "\n",
    "translation_structured_llm = translation_model.with_structured_output(Translation)\n",
    "translation_chain = translation_prompt | translation_structured_llm\n",
    "\n",
    "with BatchCallback(len(all_entries)) as cb:\n",
    "    trad_zh_translations = translation_chain.batch(all_entries, config={\"callbacks\": [cb]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5f7893",
   "metadata": {},
   "source": [
    "## Evaluation Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedcc4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestQuestion(TypedDict):\n",
    "    trad_zh_question: Annotated[str, ..., \"Question in Traditional Chinese\"]\n",
    "    trad_zh_choices: Annotated[List[str], ..., \"List of strings with possible answers to the Traditional Chinese question\"]\n",
    "    en_question: Annotated[str, ..., \"Question in English\"]\n",
    "    en_choices: Annotated[List[str], ..., \"List of strings with possible answers to the English question\"]\n",
    "    differences: Annotated[str, ..., \"Description in English explaining all dissimilarities between the Chinese and English questions and choices\"]\n",
    "    score: Annotated[int, ..., \"Numerical score ranging from 0 to 10, where zero indicates there is no relationship between the two languages and 10 indicates a perfect translation in meaning and concepts between both the questions and choices across both languages\"]\n",
    "\n",
    "system_template = \"\"\"You are a chemistry professor creating an exam that will be taken by both Taiwanese and American students. You want to ensure the exam is fair, so each of the questions asked on this exam must be a faithful representation of the same concepts within both languages. Decide whether the the following Chinese question, CHINESE_QUESTION, possible Chinese answer choices, CHINESE_CHOICES, English question, ENGLISH_QUESTION, and possible English answer choices, ENGLISH_CHOICES, are faithful representations of the same concepts.\"\"\"\n",
    "\n",
    "human_template = \"\"\"\n",
    "CHINESE_QUESTION: {trad_zh_question}\n",
    "CHINESE_CHOICES: {trad_zh_choices}\n",
    "\n",
    "ENGLISH_QUESTION: {en_question}\n",
    "ENGLISH_CHOICES: {en_choices}\n",
    "\"\"\"\n",
    "\n",
    "eval_structured_llm = evaluation_model.with_structured_output(TestQuestion)\n",
    "eval_prompt = ChatPromptTemplate.from_messages([(\"system\", system_template), (\"human\", human_template)])\n",
    "eval_chain = eval_prompt | eval_structured_llm\n",
    "\n",
    "with BatchCallback(len(trad_zh_translations)) as cb:\n",
    "    translation_evaluations = eval_chain.batch(trad_zh_translations, config={\"callbacks\": [cb]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512024bf",
   "metadata": {},
   "source": [
    "## Write Translation and Evaluation Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9127dd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"wmdp_chem_en_zh_eval.csv\", \"wt\") as fout:\n",
    "    fieldnames = [\"en_question\", \"en_choices\", \"trad_zh_question\", \"trad_zh_choices\", \"answer\", \"score\", \"differences\"]\n",
    "    writer = csv.DictWriter(fout, fieldnames=fieldnames, dialect=\"excel\")\n",
    "    writer.writeheader()\n",
    "    for eval_entry, orig_entry in zip(translation_evaluations, all_entries):\n",
    "        out = eval_entry.copy()\n",
    "        out[\"answer\"] = orig_entry[\"answer\"]\n",
    "        \n",
    "        if isinstance(out[\"en_choices\"], list):\n",
    "            out[\"en_choices\"] = json.dumps(out[\"en_choices\"], ensure_ascii=False)\n",
    "        if isinstance(out[\"trad_zh_choices\"], list):   \n",
    "            out[\"trad_zh_choices\"] = json.dumps(out[\"trad_zh_choices\"], ensure_ascii=False)\n",
    "            \n",
    "        # The LLM fixes the spelling errors in the data ...\n",
    "        if eval_entry[\"en_question\"] != orig_entry[\"en_question\"]:\n",
    "            print(\"Different question:\")\n",
    "            print(\"New: \" + eval_entry[\"en_question\"])\n",
    "            print(\"Old: \" + orig_entry[\"en_question\"])\n",
    "            print(\"\")\n",
    "            \n",
    "        if repr(eval(out[\"en_choices\"])) != repr(eval(orig_entry[\"en_choices\"])):\n",
    "            print(\"Different choices:\")\n",
    "            print(\"New: \" + repr(out[\"en_choices\"]))\n",
    "            print(\"Old: \" + repr(orig_entry[\"en_choices\"]))\n",
    "            print(\"\")\n",
    "            \n",
    "        writer.writerow(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbf72d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = [eval_entry[\"score\"] for eval_entry in translation_evaluations]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(all_scores, bins=range(12))\n",
    "ax.set_xlabel(\"Scores\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"Histogram of Evaluation Scores\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5176ecda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
